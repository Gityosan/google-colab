{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kgnOijt3M5A6",
        "ezlvBruHUPFG"
      ],
      "authorship_tag": "ABX9TyMQGNxkPsKl4oArCasDtRBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gityosan/google-colab/blob/main/step2-ver1-2-kmeansClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インストール初期設定等"
      ],
      "metadata": {
        "id": "td1BwvSSoG8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q japanize_matplotlib # matplotlib numpy plotly networkx sklearn tqdm はプリインストール済み\n",
        "!python -V\n",
        "!rm -rf sample_data/\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kNj9eIRfGSJ",
        "outputId": "fe6436b3-8f86-4a81-92ba-2d150e49c9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/4.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/4.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for japanize_matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Python 3.8.16\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 標準ライブラリ\n",
        "import gc\n",
        "import glob\n",
        "import itertools\n",
        "import logging\n",
        "import operator\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import unicodedata\n",
        "\n",
        "import japanize_matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx  # グラフ/ネットワーク理論系の計算を行うためのPythonのパッケージ\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "import plotly.io as pio\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import (\n",
        "    calinski_harabasz_score,\n",
        "    silhouette_samples,\n",
        "    silhouette_score,\n",
        ")\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "tZpQiGIkuIqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { vertical-output: true, form-width: \"35%\", display-mode: \"both\" }\n",
        "\n",
        "# 初期設定群\n",
        "target_words = [\n",
        "    \"失笑\",\n",
        "    \"なし崩し\",\n",
        "    \"なしくずし\",\n",
        "    \"御の字\",\n",
        "    \"すべからく\",\n",
        "    \"割愛\",\n",
        "    \"破天荒\",\n",
        "    \"役不足\",\n",
        "    \"確信犯\",\n",
        "    \"炎上\",\n",
        "    \"草\",\n",
        "]\n",
        "target_words = [unicodedata.normalize(\"NFC\", w) for w in target_words]\n",
        "base_dir = \"drive/MyDrive/script/bert/\"  # @param {type:\"string\"}\n",
        "path_list = glob.glob(base_dir + \"word-vectors/1-512/*.dict\")\n",
        "path_list_map = {w: list(filter(lambda x: w in x, path_list)) for w in target_words}"
      ],
      "metadata": {
        "id": "0UJYgU0zpVLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 関数群"
      ],
      "metadata": {
        "id": "SUUoU1fho8T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 関数群\n",
        "def best_kmeans(X, max_range=np.arange(2, 11), criterion=\"silhouette\", SEED=42):\n",
        "    \"\"\"\n",
        "    Return the best K-Means clustering given the data, a range of K values, and a K-selection criterion.\n",
        "\n",
        "    :param X: usage matrix (made of usage vectors)\n",
        "    :param max_range: range within the number of clusters should lie\n",
        "    :param criterion: K-selection criterion: 'silhouette' or 'calinski'\n",
        "    :return: best_model: KMeans model (sklearn.cluster.Kmeans) with best clustering according to the criterion\n",
        "              scores: list of tuples (k, s) indicating the clustering score s obtained using k clusters\n",
        "    \"\"\"\n",
        "    assert criterion in [\"silhouette\", \"calinski\", \"harabasz\", \"calinski-harabasz\"]\n",
        "    best_n_cluster, best_model, best_score = 1, None, -1\n",
        "    scores = []\n",
        "    # クラスター数2から11までの間で最もシルエットスコアが高いものを選択\n",
        "    for k in max_range:\n",
        "        # print(\"n_cluster: {}\".format(k))\n",
        "        if k >= X.shape[0]:\n",
        "            continue\n",
        "        # クラスター数に応じてKmeansを行う\n",
        "        kmeans = KMeans(n_clusters=k, random_state=SEED)\n",
        "        clusters = kmeans.fit_predict(X)\n",
        "        # show_silhouette(kmeans, X)\n",
        "        # シルエットスコアを算出\n",
        "        if criterion == \"silhouette\":\n",
        "            score = silhouette_score(X, clusters)\n",
        "        else:\n",
        "            score = calinski_harabasz_score(X, clusters)\n",
        "\n",
        "        scores.append((k, score))\n",
        "\n",
        "        # if two clusterings yield the same score, keep the one that results from a smaller K\n",
        "        if score > best_score:\n",
        "            best_n_cluster, best_model, best_clusters = k, kmeans, clusters\n",
        "    print(\"best_n_cluster: {}\".format(best_n_cluster))\n",
        "    return best_n_cluster, best_model, best_clusters\n",
        "\n",
        "\n",
        "def cluster_usages(\n",
        "    Uw, method=\"kmeans\", k_range=np.arange(2, 11), criterion=\"silhouette\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Return the best clustering model for a usage matrix.\n",
        "\n",
        "    :param Uw: usage matrix\n",
        "    :param method: K-Means or Gaussian Mixture Model ('kmeans' or 'gmm')\n",
        "    :param k_range: range of possible K values (number of clusters)\n",
        "    :param criterion: K selection criterion; depends on clustering method\n",
        "    :return: best clustering model\n",
        "    \"\"\"\n",
        "    # standardize usage matrix by removing the mean and scaling to unit variance\n",
        "    X = preprocessing.StandardScaler().fit_transform(Uw)\n",
        "\n",
        "    # get best model according to a K-selection criterion\n",
        "    if method == \"kmeans\":\n",
        "        best_n_cluster, best_model, best_clusters = best_kmeans(\n",
        "            X, k_range, criterion=criterion\n",
        "        )\n",
        "    # elif method == 'gmm':\n",
        "    #     best_model_aic, best_model_bic, _, _ = best_gmm(X, k_range)\n",
        "    #     if criterion == 'aic':\n",
        "    #         best_model = best_model_aic\n",
        "    #     elif criterion == 'bic':\n",
        "    #         best_model = best_model_bic\n",
        "    #     else:\n",
        "    #         raise ValueError('Invalid criterion {}. Choose \"aic\" or \"bic\".'.format(criterion))\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'Invalid method \"{}\". Choose \"kmeans\" or \"gmm\".'.format(method)\n",
        "        )\n",
        "\n",
        "    return best_n_cluster, best_model, best_clusters\n",
        "\n",
        "\n",
        "def show_scatter_plot(words=[], vectors=[], best_clusters=[], output_path=\"\"):\n",
        "    # 次元削減\n",
        "    tsne = TSNE(random_state=0, perplexity=30, learning_rate=500).fit_transform(\n",
        "        np.array(vectors)\n",
        "    )\n",
        "    # 表示\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(40, 40), tight_layout=True)\n",
        "    cmap = plt.get_cmap(\"Dark2\")\n",
        "    # vocabとその位置に対応したクラスター番号を取り出す\n",
        "    for idx, word in enumerate(words):\n",
        "        # クラスターごとの色の指定\n",
        "        cval = cmap(best_clusters[idx])\n",
        "        # 各単語のベクトルを取得\n",
        "        ax.scatter(tsne[idx, 0], tsne[idx, 1], marker=\".\", color=cval)\n",
        "        # 単語を表示\n",
        "        if word == usages[\"target_word\"][\"word\"]:\n",
        "            ax.annotate(word, xy=(tsne[idx, 0], tsne[idx, 1]), size=50, color=cval)\n",
        "        else:\n",
        "            ax.annotate(word, xy=(tsne[idx, 0], tsne[idx, 1]), color=cval)\n",
        "    plt.savefig(output_path)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "# 横軸をシルエット係数、縦軸をクラスター番号としてプロットし、シルエット分析を可視化する関数\n",
        "def show_silhouette(fitted_model, vectors=[]):\n",
        "    cluster_labels = np.unique(fitted_model.labels_)\n",
        "    num_cluster = cluster_labels.shape[0]\n",
        "    # シルエット係数の計算\n",
        "    silhouette_vals = silhouette_samples(vectors, fitted_model.labels_)\n",
        "    # 可視化\n",
        "    y_ax_lower, y_ax_upper = 0, 0\n",
        "    y_ticks = []\n",
        "\n",
        "    for idx, cls in enumerate(cluster_labels):\n",
        "        cls_silhouette_vals = silhouette_vals[fitted_model.labels_ == cls]\n",
        "        cls_silhouette_vals.sort()\n",
        "        y_ax_upper += len(cls_silhouette_vals)\n",
        "        cmap = cm.get_cmap(\"Spectral\")\n",
        "        # rgbaの配列\n",
        "        rgba = list(cmap(idx / num_cluster))\n",
        "        # alpha値を0.7にする\n",
        "        rgba[-1] = 0.7\n",
        "        plt.barh(\n",
        "            y=range(y_ax_lower, y_ax_upper),\n",
        "            width=cls_silhouette_vals,\n",
        "            height=1.0,\n",
        "            edgecolor=\"none\",\n",
        "            color=rgba,\n",
        "        )\n",
        "        y_ticks.append((y_ax_lower + y_ax_upper) / 2.0)\n",
        "        y_ax_lower += len(cls_silhouette_vals)\n",
        "\n",
        "    silhouette_avg = np.mean(silhouette_vals)\n",
        "    # 各クラスターのシルエット係数の平均をプロット\n",
        "    plt.axvline(silhouette_avg, color=\"orangered\")\n",
        "    plt.xlabel(\"sihouette coefficient\")\n",
        "    plt.ylabel(\"cluster\")\n",
        "    # クラスター番号をプロット\n",
        "    plt.yticks(y_ticks, cluster_labels + 1)\n",
        "    plt.show()\n",
        "\n",
        "def log_word_same_cluster(tuple_list): #list(zip(ar1,ar2))\n",
        "    relative_words = {}\n",
        "    for id in set([v[0] for v in tuple_list]):\n",
        "        relative_words[id] = [w[1] for w in filter(lambda x: id == x[0], tuple_list)]\n",
        "    for words in relative_words.values():\n",
        "        print(words[:20])"
      ],
      "metadata": {
        "id": "8TzGnVmuv0n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主要処理"
      ],
      "metadata": {
        "id": "CyW0KMuZnTK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in path_list_map:\n",
        "    print(\"-----------------\")\n",
        "    print(\"Start with {}\".format(key))\n",
        "    path_list = path_list_map[key]\n",
        "    if not len(path_list):\n",
        "        continue\n",
        "    path_list.sort()\n",
        "    output_path = (\n",
        "        base_dir\n",
        "        + \"transition-plot/\"\n",
        "        + \"/\".join(path_list[0].split(\"/\")[-2:]).split(\".\")[-2]\n",
        "        + \".png\"\n",
        "    )\n",
        "    # check outputs\n",
        "    if os.path.exists(\"/content/\" + output_path):\n",
        "        print(\"Exit from function because the file already exists\")\n",
        "        continue\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    # loading data\n",
        "    embeddings = []\n",
        "    for path in path_list:\n",
        "        with open(path, \"rb\") as f:\n",
        "            binary = pickle.load(f)\n",
        "            embeddings.append(\n",
        "                {\"filename\": path.split(\"/\")[-1].split(\".\")[0], \"usage\": binary[\"target_word\"]}\n",
        "            )\n",
        "\n",
        "    # 次元削除\n",
        "    tsne = TSNE(random_state=0, perplexity=30, learning_rate=500).fit_transform(\n",
        "        np.array([u[\"usage\"][\"vector\"] for u in embeddings])\n",
        "    )\n",
        "    # 表示\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(30, 20), tight_layout=True)\n",
        "    cmap = plt.get_cmap(\"Dark2\")\n",
        "    for idx, u in enumerate(embeddings):\n",
        "        # 各単語のベクトルを取得\n",
        "        ax.scatter(tsne[idx, 0], tsne[idx, 1], 10, marker=\".\", color=cmap(0))\n",
        "        # 単語を表示\n",
        "        ax.annotate(u[\"filename\"], xy=(tsne[idx, 0], tsne[idx, 1]), size=20, color=cmap(0))\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zcq1jYPRpH_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in path_list_map:\n",
        "    for path in path_list_map[key]:\n",
        "        print(\"-----------------\")\n",
        "        print(\"Start with {}\".format(path.split(\"/\")[-1]))\n",
        "        # check outputs\n",
        "        output_path = base_dir + \"scatter-plot/\" + \"/\".join(path.split(\"/\")[-2:]) + \".png\"\n",
        "        # output_path = base_dir + \"scatter-plot/\" + \"/\".join(path.split(\"/\")[-2:]).split(\".\")[-2] + \".png\"\n",
        "        if os.path.exists(\"/content/\" + output_path):\n",
        "            print(\"Exit from function because the file already exists\")\n",
        "            continue\n",
        "        with open(path, \"rb\") as f:\n",
        "            usages = pickle.load(f)\n",
        "            usages[\"all\"] = list(filter(lambda x: '#' not in x[\"word\"], usages[\"all\"]))\n",
        "        best_n_cluster, best_model, best_clusters = cluster_usages(\n",
        "            Uw=[u[\"vector\"] for u in usages[\"all\"]],\n",
        "            method=\"kmeans\",\n",
        "            k_range=np.arange(2, 11),\n",
        "            criterion=\"silhouette\",\n",
        "        )\n",
        "        log_word_same_cluster(\n",
        "            list(zip(best_model.labels_, [u[\"word\"] for u in usages[\"all\"]]))\n",
        "        )\n",
        "        show_scatter_plot(\n",
        "            [u[\"word\"] for u in usages[\"all\"]],\n",
        "            [u[\"vector\"] for u in usages[\"all\"]],\n",
        "            best_clusters,\n",
        "            output_path,\n",
        "        )"
      ],
      "metadata": {
        "id": "lLa2FzifF2Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保存後処理"
      ],
      "metadata": {
        "id": "kgnOijt3M5A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# formatter\n",
        "!pip install -q black[jupyter]\n",
        "!black \"/content/drive/MyDrive/Colab Notebooks/bertClustering.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901d471e-0fe9-48cd-eb4e-a32185c2d764",
        "id": "pkCcMEOnM_PF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### その他"
      ],
      "metadata": {
        "id": "ezlvBruHUPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# RAMのサイズをcheck\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(\"Your runtime has {:.1f} gigabytes of available RAM\\n\".format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "    print(\"You are using a high-RAM runtime!\")\n",
        "# GPUの数をcheck\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
        "# Check GPU recognized\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d010c7-0dac-4ffe-f479-515c2e94356d",
        "id": "KyONTzTQUY6H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Num GPUs Available:  1\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6354157854472975992\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11586961408\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14767642777879333371\n",
            "physical_device_desc: \"device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    }
  ]
}