{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wrwv_yafBpkP"
      ],
      "authorship_tag": "ABX9TyMFukZZSMhC8v1AUX80XwDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gityosan/google-colab/blob/main/step1-ver1-1-twitterSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- なし崩し\n",
        "- 失笑\n",
        "- 御の字\n",
        "- 姑息\n",
        "- すべからく\n",
        "- 割愛\n",
        "- 破天荒\n",
        "- 役不足\n",
        "- 確信犯\n",
        "\n",
        "\n",
        "- https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
        "- https://developer.twitter.com/ja/docs/twitter-api/rate-limits#v2-limits\n"
      ],
      "metadata": {
        "id": "WxrjGqfnt-Pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インストール・初期設定等"
      ],
      "metadata": {
        "id": "wrwv_yafBpkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V\n",
        "!rm -rf sample_data/\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw7EIGxKBYZC",
        "outputId": "b37734f1-1098-43f5-8d3d-98bdd4613bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRr8uBOlzj0R"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global settings\n",
        "os.environ[\"TOKEN\"] = \"AAAAAAAAAAAAAAAAAAAAANV8iQEAAAAAH8KguG0Tdayki1U7cJK9Tif1mEY%3DxAy9AVQPDwYX7kj1fyP7fgfZFeFzEKD6TLTx0N70I6muHHFn8p\"\n",
        "target_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
        "base_dir = \"drive/MyDrive/script/bert/\"\n",
        "if not os.path.exists(\"response_count.json\"):\n",
        "    with open(\"response_count.json\", \"w\", encoding='utf-8') as f:\n",
        "        json.dump({\"meta\":{\"words\":[]}}, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "44njLW2ApneK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_header():\n",
        "  return {\"Authorization\": \"Bearer {}\".format(os.getenv(\"TOKEN\"))}\n",
        "\n",
        "def create_params(keyword, start_time, end_time, max_results = 10, next_token = None):\n",
        "  #change params based on the endpoint you are using\n",
        "  return {'query': keyword,\n",
        "          'start_time': start_time,\n",
        "          'end_time': end_time,\n",
        "          'max_results': max_results,\n",
        "          'expansions': '',\n",
        "          'tweet.fields': 'id,text,created_at',\n",
        "          'next_token': next_token}\n",
        "\n",
        "def request_to_endpoint(url, headers, params):\n",
        "  response = requests.request(\"GET\", url, headers=headers, params=params)\n",
        "  time.sleep(1)\n",
        "  # print(\"Endpoint Response Code: \" + str(response.status_code))\n",
        "  if response.status_code != 200:\n",
        "      raise Exception(response.status_code, response.text)\n",
        "  return response.json()\n",
        "\n",
        "def fetchTweet(keyword, start_time, end_time):\n",
        "  rest = int(max_results)\n",
        "  response = {\"data\": [], \"size\": 0}\n",
        "  next_token = None\n",
        "  while rest > 0:\n",
        "    res = request_to_endpoint(url=target_url, headers=create_header(), params=create_params(keyword, start_time, end_time, max_results=500, next_token=next_token))\n",
        "    rest -= 500\n",
        "    if \"meta\" in res:\n",
        "      response[\"size\"] += int(res['meta']['result_count'])\n",
        "      if \"next_token\" in res[\"meta\"]:\n",
        "        next_token = res[\"meta\"][\"next_token\"]\n",
        "    if \"data\" in res:\n",
        "      response[\"data\"].extend(res[\"data\"])\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "4TreYIa3pnnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主要処理"
      ],
      "metadata": {
        "id": "7isHaxVGBmiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 単一キーワード取得\n",
        "keyword = \"失笑\"\n",
        "year=2014\n",
        "max_results = 10000\n",
        "\n",
        "word = keyword + \" lang:ja\"\n",
        "start_time = str(year)+\"-01-01T00:00:00.000Z\"\n",
        "end_time = str(year)+\"-12-31T00:00:00.000Z\"\n",
        "with open(\"{}.json\".format('only-'+str(keyword)+'-'+str(year)), \"w\", encoding='utf-8') as f:\n",
        "  json.dump(fetchTweet(word, start_time, end_time), f, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "15dZvnAZw-9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 複数キーワード取得\n",
        "keywords = [\"失笑\",\"なし崩し\",\"なしくずし\",\"御の字\",\"姑息\",\"須く\",\"すべからく\",\"割愛\",\"破天荒\",\"役不足\",\"確信犯\",\"炎上\",\"草\"]\n",
        "years = list(range(2007,2021))\n",
        "max_results = 10000\n",
        "\n",
        "\n",
        "for k in keywords:\n",
        "  word = k + \" lang:ja\"\n",
        "  for y in years:\n",
        "    with open(\"response_count.json\") as response_count_json:\n",
        "      response_count = json.load(response_count_json)\n",
        "      if str(k) in response_count and str(y) in response_count[str(k)]: continue; pass\n",
        "      start_time = str(y)+\"-01-01T00:00:00.000Z\"\n",
        "      end_time = str(y)+\"-12-31T00:00:00.000Z\"\n",
        "      response = fetchTweet(word, start_time, end_time)\n",
        "      if str(k) in response_count:\n",
        "        response_count[str(k)][str(y)]= response[\"size\"]\n",
        "      else:\n",
        "        response_count['meta']['words'].append(str(k))\n",
        "        response_count[str(k)] = {str(y): response[\"size\"]}\n",
        "      with open(\"response_count.json\", \"w\", encoding='utf-8') as f:\n",
        "        json.dump(response_count, f, ensure_ascii=False)\n",
        "      with open(base_dir+\"twitter-corpus/{}.json\".format(str(k)+'-'+str(y)), \"w\", encoding='utf-8') as f:\n",
        "        json.dump(response, f, ensure_ascii=False)\n",
        "      print('fetch finish:'+str(k)+':'+str(y))"
      ],
      "metadata": {
        "id": "WfcTJsZXpvWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9082721-80bf-4548-e764-00267b8e56a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fetch finish:確信犯:2015\n",
            "fetch finish:確信犯:2016\n",
            "fetch finish:確信犯:2017\n",
            "fetch finish:確信犯:2018\n",
            "fetch finish:確信犯:2019\n",
            "fetch finish:確信犯:2020\n"
          ]
        }
      ]
    }
  ]
}