{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOOpiMyHQdbROs3DpaFRI6u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers ipadic fugashi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBSZfJpPNW4C",
        "outputId": "3444893e-a3a2-42b5-dd33-8002c58a85da"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.8/dist-packages (1.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BertJapaneseTokenizer, BertModel"
      ],
      "metadata": {
        "id": "zKuRtNx-bJIF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Hh5jjwZkrgPJ"
      },
      "outputs": [],
      "source": [
        "# 関数群\n",
        "# 前処理\n",
        "def preprocessing(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@[w/:%#$&?()~.=+-…]+[:]? \", \"\", text)\n",
        "    text = re.sub(r\"(^RT )\", \"\", text)\n",
        "    text = text.lower()  # 小文字化\n",
        "    text = re.sub(\"\\r\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(\"\\n\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # 数字列をdelete\n",
        "    ZEN = \"\".join(chr(0xFF01 + i) for i in range(94))  # 全角記号一覧\n",
        "    HAN = \"\".join(chr(0x21 + i) for i in range(94))  # 半角記号一覧\n",
        "    ETC = \"\".join(chr(0x3000 + i) for i in range(30))  # その他主要そうな記号\n",
        "    text = text.translate(str.maketrans(ZEN, HAN))  # 全角記号を半角記号に置換\n",
        "    FIXED_HAN = re.sub(r\"[\\w]+\", \"\", HAN)\n",
        "    return re.sub(\"[\" + \"~\" + \"*\" + \"＊\" + ETC + FIXED_HAN + \"]\", \" \", text)  # 記号を消す\n",
        "\n",
        "\n",
        "# JSONからdf作成\n",
        "def JSONtoDF(path):\n",
        "    with open(path) as f:\n",
        "        j = json.load(f)\n",
        "        return pd.json_normalize(data=j[\"data\"]).drop(\n",
        "            [\"created_at\", \"edit_history_tweet_ids\"], axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "def prepareCorpus(path_list):\n",
        "    res = pd.DataFrame()\n",
        "    for path in path_list:\n",
        "        df = JSONtoDF(path)\n",
        "        df[\"preprocessedText\"] = [preprocessing(text) for text in df[\"text\"]]\n",
        "        res = pd.concat([res, df])\n",
        "    return res.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def get_context(token_ids, target_position, sequence_length=128):\n",
        "    \"\"\"\n",
        "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
        "    (and the target word's position in the snippet).\n",
        "\n",
        "    :param token_ids: list of token ids (for an entire line of text)\n",
        "    :param target_position: index of the target word's position in `tokens`\n",
        "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
        "    :return: (context_ids, new_target_position)\n",
        "                context_ids: list of token ids for the output sequence\n",
        "                new_target_position: index of the target word's position in `context_ids`\n",
        "    \"\"\"\n",
        "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
        "    window_size = int((sequence_length - 2) / 2)\n",
        "    context_start = max([0, target_position - window_size])\n",
        "    padding_offset = max([0, window_size - target_position])\n",
        "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
        "    context_ids = token_ids[context_start : target_position + window_size]\n",
        "    context_ids += padding_offset * [0]\n",
        "    new_target_position = target_position - context_start\n",
        "    return context_ids, new_target_position\n",
        "\n",
        "\n",
        "def get_usage(\n",
        "    text_list=[],\n",
        "    target_word=None,\n",
        "    output_path=\"word-vectors/{}.dict\".format(random.randrange(1000, 10000)),\n",
        "    sequence_length=256,\n",
        "    buffer_size=512,\n",
        "    layer_range=(1, 14),\n",
        "):\n",
        "    if not target_word:\n",
        "        return\n",
        "    TW_token = tokenizer.encode(target_word)\n",
        "    TW_token = TW_token[1 : len(TW_token) - 1]\n",
        "    batches = []\n",
        "    for text in text_list:\n",
        "        tokens = tokenizer.encode(text)\n",
        "        start_index = tokens.index(TW_token[0]) if TW_token[0] in tokens else None\n",
        "        if start_index == None:\n",
        "            continue\n",
        "        if TW_token == tokens[start_index : start_index + len(TW_token)]:\n",
        "            input_ids, position = get_context(tokens, start_index, sequence_length)\n",
        "            batches.append(\n",
        "                {\n",
        "                    \"word\": target_word,\n",
        "                    \"position\": (position, position + len(TW_token)),\n",
        "                    \"input_ids\": input_ids,\n",
        "                }\n",
        "            )\n",
        "            if len(batches) >= buffer_size:\n",
        "                break\n",
        "    print(\"Start model fit\")\n",
        "    with torch.no_grad():\n",
        "        input_ids_tensor = torch.tensor([b[\"input_ids\"] for b in batches])\n",
        "        outputs = model(input_ids_tensor, output_hidden_states=True)\n",
        "    hidden_states = [l.clone().numpy() for l in outputs[2]]\n",
        "    # outputs[2]は三次元のtensor型が13個並ぶ配列 (13, 512, 256, 768) (13, B, |s|, 768)\n",
        "    # 13(隠れ12層+最終層)×文章数×単語数×768次元になる\n",
        "    print(\"Finish model fit: word: {} / size: {}\".format(target_word, len(batches)))\n",
        "\n",
        "    # defaultで12層すべての和をとる\n",
        "    usage_vectors = np.sum(\n",
        "        np.stack(hidden_states)[layer_range[0] : layer_range[1], :, :, :], axis=0\n",
        "    )\n",
        "    usages = []\n",
        "    # すでにファイルが存在すれば続きから追記\n",
        "    if os.path.exists(output_path):\n",
        "        with open(output_path, \"rb\") as f:\n",
        "            usages = pickle.load(f)\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    for i, b in enumerate(batches):\n",
        "        b[\"vector\"] = np.sum(\n",
        "            usage_vectors[i, b[\"position\"][0] : b[\"position\"][1], :], axis=0\n",
        "        )  # 文章の分散表現\n",
        "    usages.extend(batches)\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        pickle.dump(usages, f)\n",
        "\n",
        "\n",
        "# 未実施処理\n",
        "# - Japanese.txtに基づくストップワード除去\n",
        "# - 重複Tweet削除"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期設定群\n",
        "target = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(target)\n",
        "model = BertModel.from_pretrained(target)\n",
        "\n",
        "target_words = [\n",
        "    \"失笑\",\n",
        "    \"なし崩し\",\n",
        "    \"なしくずし\",\n",
        "    \"御の字\",\n",
        "    # \"姑息\",\n",
        "    \"すべからく\",\n",
        "    \"割愛\",\n",
        "    \"破天荒\",\n",
        "    \"役不足\",\n",
        "    \"確信犯\",\n",
        "    \"炎上\",\n",
        "    \"草\",\n",
        "]\n",
        "\n",
        "\n",
        "data_dir = \"twitter-corpus\"\n",
        "path_list = glob.glob(data_dir + \"/*.json\")\n",
        "path_list.sort()\n",
        "# ターゲットとする単語や期間の整理は予めここで行う\n",
        "oparation = [\n",
        "    {\n",
        "        \"word\": w,\n",
        "        \"year\": y,\n",
        "        \"path_list\": list(filter(lambda x: w in x and str(y) in x, path_list)),\n",
        "    }\n",
        "    for w in target_words\n",
        "    for y in range(2007, 2021)\n",
        "]\n",
        "for o in oparation:\n",
        "    if not len(o[\"path_list\"]):\n",
        "        continue\n",
        "    output_path = \"word-vectors/\" + o[\"word\"] + \"-\" + str(o[\"year\"]) + \".dict\"\n",
        "    corpus = prepareCorpus(o[\"path_list\"])\n",
        "    get_usage(\n",
        "        text_list=corpus[\"preprocessedText\"].values.tolist(),\n",
        "        target_word=o[\"word\"],\n",
        "        output_path=output_path,\n",
        "        sequence_length=256,\n",
        "        buffer_size=512,\n",
        "        layer_range=(1, 14),\n",
        "    )"
      ],
      "metadata": {
        "id": "w_xsCuP8UdCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4ea710-671f-4853-db90-ba328c3b9f21"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 失笑 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 410\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なし崩し / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 80\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 430\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: なしくずし / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 300\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 御の字 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: すべからく / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 300\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 割愛 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 350\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 破天荒 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 200\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 役不足 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 確信犯 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 炎上 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n",
            "Start model fit\n",
            "Finish model fit: word: 草 / size: 512\n"
          ]
        }
      ]
    }
  ]
}