{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgudlMC8L/lpcQHGSaKY/1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers ipadic fugashi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBSZfJpPNW4C",
        "outputId": "4fee782d-00be-4fa0-ccc4-7f92029320c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.8/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BertJapaneseTokenizer, BertModel"
      ],
      "metadata": {
        "id": "zKuRtNx-bJIF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hh5jjwZkrgPJ"
      },
      "outputs": [],
      "source": [
        "# 関数群\n",
        "# 前処理\n",
        "def preprocessing(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@[w/:%#$&?()~.=+-…]+[:]? \", \"\", text)\n",
        "    text = re.sub(r\"(^RT )\", \"\", text)\n",
        "    text = text.lower()  # 小文字化\n",
        "    text = re.sub(\"\\r\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(\"\\n\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # 数字列をdelete\n",
        "    ZEN = \"\".join(chr(0xFF01 + i) for i in range(94))  # 全角記号一覧\n",
        "    HAN = \"\".join(chr(0x21 + i) for i in range(94))  # 半角記号一覧\n",
        "    ETC = \"\".join(chr(0x3000 + i) for i in range(30))  # その他主要そうな記号\n",
        "    text = text.translate(str.maketrans(ZEN, HAN))  # 全角記号を半角記号に置換\n",
        "    FIXED_HAN = re.sub(r\"[\\w]+\", \"\", HAN)\n",
        "    return re.sub(\"[\" + \"~\" + \"*\" + \"＊\" + ETC + FIXED_HAN + \"]\", \" \", text)  # 記号を消す\n",
        "\n",
        "\n",
        "# JSONからdf作成\n",
        "def JSONtoDF(path):\n",
        "    with open(path) as f:\n",
        "        j = json.load(f)\n",
        "        return pd.json_normalize(data=j[\"data\"]).drop(\n",
        "            [\"created_at\", \"edit_history_tweet_ids\"], axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "def prepareCorpus(path_list):\n",
        "    res = pd.DataFrame()\n",
        "    for path in path_list:\n",
        "        df = JSONtoDF(path)\n",
        "        df[\"preprocessedText\"] = [preprocessing(text) for text in df[\"text\"]]\n",
        "        res = pd.concat([res, df])\n",
        "    return res.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def get_context(token_ids, target_position, sequence_length=128):\n",
        "    \"\"\"\n",
        "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
        "    (and the target word's position in the snippet).\n",
        "\n",
        "    :param token_ids: list of token ids (for an entire line of text)\n",
        "    :param target_position: index of the target word's position in `tokens`\n",
        "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
        "    :return: (context_ids, new_target_position)\n",
        "                context_ids: list of token ids for the output sequence\n",
        "                new_target_position: index of the target word's position in `context_ids`\n",
        "    \"\"\"\n",
        "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
        "    window_size = int((sequence_length - 2) / 2)\n",
        "    context_start = max([0, target_position - window_size])\n",
        "    padding_offset = max([0, window_size - target_position])\n",
        "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
        "    context_ids = token_ids[context_start : target_position + window_size]\n",
        "    context_ids += padding_offset * [0]\n",
        "    new_target_position = target_position - context_start\n",
        "    return context_ids, new_target_position\n",
        "\n",
        "\n",
        "def get_i2w(target_words):\n",
        "    i2w = {}\n",
        "    for t in target_words:\n",
        "        t_id = tokenizer.encode(t)[1]\n",
        "        if t_id:\n",
        "            i2w[t_id] = t\n",
        "    return i2w\n",
        "\n",
        "\n",
        "def get_usage(\n",
        "    text_list=[],\n",
        "    i2w={},\n",
        "    output_path=\"word-vectors/{}.dict\".format(random.randrange(1000, 10000)),\n",
        "    sequence_length=256,\n",
        "    buffer_size=512,\n",
        "    layer_range=(1, 14),\n",
        "):\n",
        "    batches = []\n",
        "    for text in text_list:\n",
        "        tokens = tokenizer.encode(text)\n",
        "        for index, token in enumerate(tokens):\n",
        "            if token not in i2w:\n",
        "                continue\n",
        "            context_ids, position = get_context(tokens, index, sequence_length)\n",
        "            batches.append(\n",
        "                {\n",
        "                    \"word\": i2w[token],\n",
        "                    \"position\": position,\n",
        "                    \"input_ids\": [101] + context_ids + [102],\n",
        "                    \"context_ids\": context_ids,\n",
        "                }\n",
        "            )\n",
        "            if len(batches) >= buffer_size:\n",
        "                break\n",
        "        else:\n",
        "            continue\n",
        "        break\n",
        "\n",
        "    print(\"=====================\")\n",
        "    print(\n",
        "        \"{} data_size: {}\".format(\n",
        "            output_path.split(\"/\")[-1].split(\".\")[0], len(batches)\n",
        "        )\n",
        "    )\n",
        "    print(\"=====================\")\n",
        "\n",
        "    # print(\"Start model fit\")\n",
        "    # hidden_states = torch.Tensor()\n",
        "    # input_ids_tensor = torch.tensor([b[\"input_ids\"] for b in batches])\n",
        "    # for inid in tqdm(input_ids_tensor[:]):\n",
        "    #     with torch.no_grad():\n",
        "    #         outputs = model(inid.reshape(1, -1), output_hidden_states=True)\n",
        "    #     hidden_states = torch.cat((hidden_states, outputs.last_hidden_state))\n",
        "    # print(\"Finish model fit\")\n",
        "\n",
        "    print(\"Start model fit\")\n",
        "    with torch.no_grad():\n",
        "        input_ids_tensor = torch.tensor([b[\"input_ids\"] for b in batches])\n",
        "        outputs = model(input_ids_tensor, output_hidden_states=True)\n",
        "    hidden_states = [l.clone().numpy() for l in outputs[2]]\n",
        "    # outputs[2]は三次元のtensor型が13個並ぶ配列 (13, 2, 256, 768) (13, B, |s|, 768)\n",
        "    # 13(隠れ12層+最終層)×文章数×768次元になる\n",
        "    print(\"Finish model fit\")\n",
        "\n",
        "    # defaultで12層すべての和をとる\n",
        "    usage_vectors = np.sum(\n",
        "        np.stack(hidden_states)[layer_range[0] : layer_range[1], :, :], axis=0\n",
        "    )\n",
        "    usages = {}\n",
        "    # すでにファイルが存在すれば続きから追記\n",
        "    if os.path.exists(output_path):\n",
        "        with open(output_path, \"rb\") as f:\n",
        "            usages = pickle.load(f)\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    for i, b in enumerate(batches):\n",
        "        # int(b[\"position\"])に+1されているのは、スペシャルトークンの分だけターゲットワードの位置が後ろに一個分ズレているから\n",
        "        usage_vector = usage_vectors[i, int(b[\"position\"]) + 1]  # 文章の分散表現\n",
        "        if b[\"word\"] not in usages:\n",
        "          usages[b[\"word\"]]=[]\n",
        "        usages[b[\"word\"]].append(\n",
        "            {\n",
        "                \"word\": b[\"word\"],\n",
        "                \"position\": b[\"position\"],\n",
        "                \"input_ids\": b[\"input_ids\"],\n",
        "                \"context_ids\": b[\"context_ids\"],\n",
        "                \"vector\": usage_vector,\n",
        "            }\n",
        "        )\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        pickle.dump(usages, f)\n",
        "\n",
        "\n",
        "# 未実施処理\n",
        "# - Japanese.txtに基づくストップワード除去\n",
        "# - 重複Tweet削除"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期設定群\n",
        "target = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(target)\n",
        "model = BertModel.from_pretrained(target)\n",
        "\n",
        "target_words = [\n",
        "    \"失笑\",\n",
        "    \"なし崩し\",\n",
        "    \"なしくずし\",\n",
        "    \"御の字\",\n",
        "    \"姑息\",\n",
        "    \"すべからく\",\n",
        "    \"割愛\",\n",
        "    \"破天荒\",\n",
        "    \"役不足\",\n",
        "    \"確信犯\",\n",
        "    \"炎上\",\n",
        "    \"草\",\n",
        "]\n",
        "\n",
        "\n",
        "data_dir = \"twitter-corpus\"\n",
        "path_list = glob.glob(data_dir + \"/*.json\")\n",
        "for path in path_list:\n",
        "    output_path = \"word-vectors/\" + path.split(\"/\")[-1].split(\".\")[0] + \".dict\"\n",
        "    corpus = prepareCorpus([path])\n",
        "    get_usage(\n",
        "        text_list=corpus[\"preprocessedText\"].values.tolist(),\n",
        "        i2w=get_i2w(target_words),\n",
        "        output_path=output_path,\n",
        "        sequence_length=256,\n",
        "        buffer_size=378,\n",
        "        layer_range=(1, 14),\n",
        "    )"
      ],
      "metadata": {
        "id": "w_xsCuP8UdCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5fc7d2-4d3e-4e2e-c00a-6561bed74c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================\n",
            "確信犯-2020 data_size: 378\n",
            "=====================\n",
            "Start model fit\n",
            "Finish model fit\n"
          ]
        }
      ]
    }
  ]
}