{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gityosan/google-colab/blob/main/bertClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インストール・初期設定等"
      ],
      "metadata": {
        "id": "VFXH26A8-biY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBSZfJpPNW4C",
        "outputId": "6c66d47f-5950-469d-9b34-5aa7c1352c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers ipadic fugashi\n",
        "!python -V\n",
        "!rm -rf sample_data/\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zKuRtNx-bJIF"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BertJapaneseTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { vertical-output: true, form-width: \"35%\", display-mode: \"both\" }\n",
        "\n",
        "# 初期設定群\n",
        "target = \"cl-tohoku/bert-base-japanese-whole-word-masking\"  # @param {type:\"string\"}\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(target, tokenize_chinese_chars=False)\n",
        "model = BertModel.from_pretrained(target).to(\n",
        "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "\n",
        "target_words = [\n",
        "    \"失笑\",\n",
        "    \"なし崩し\",\n",
        "    \"なしくずし\",\n",
        "    \"御の字\",\n",
        "    # \"姑息\",\n",
        "    \"すべからく\",\n",
        "    \"割愛\",\n",
        "    \"破天荒\",\n",
        "    \"役不足\",\n",
        "    \"確信犯\",\n",
        "    \"炎上\",\n",
        "    \"草\",\n",
        "]\n",
        "base_dir = \"drive/MyDrive/script/bert/\"  # @param {type:\"string\"}\n",
        "with open(base_dir + \"Japanese.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    stop_words = set([w.strip() for w in f] + [\"する\", \"なる\", \"いる\", \"ある\"])\n",
        "\n",
        "path_list = glob.glob(base_dir + \"twitter-corpus/*.json\")\n",
        "path_list.sort()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e36287-6b17-4ce6-db7b-0e5c2c3e847e",
        "id": "g2t9xhX3A31V"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 関数群"
      ],
      "metadata": {
        "id": "_SSWFYtI-6TL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hh5jjwZkrgPJ"
      },
      "outputs": [],
      "source": [
        "# 前処理\n",
        "def preprocessing(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@[w/:%#$&?()~.=+-…]+[:]? \", \"\", text)\n",
        "    text = re.sub(r\"(^RT )\", \"\", text)\n",
        "    text = text.lower()  # 小文字化\n",
        "    text = re.sub(\"\\r\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(\"\\n\", \"\", text)  # \\r\\nをdelete\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # 数字列をdelete\n",
        "    ZEN = \"\".join(chr(0xFF01 + i) for i in range(94))  # 全角記号一覧\n",
        "    HAN = \"\".join(chr(0x21 + i) for i in range(94))  # 半角記号一覧\n",
        "    ETC = \"\".join(chr(0x3000 + i) for i in range(30))  # その他主要そうな記号\n",
        "    text = text.translate(str.maketrans(ZEN, HAN))  # 全角記号を半角記号に置換\n",
        "    FIXED_HAN = re.sub(r\"[\\w]+\", \"\", HAN)\n",
        "    return re.sub(\"[\" + \"~\" + \"*\" + \"＊\" + ETC + FIXED_HAN + \"]\", \" \", text)  # 記号を消す\n",
        "\n",
        "\n",
        "# JSONからdf作成\n",
        "def JSONtoDF(path):\n",
        "    with open(path) as f:\n",
        "        j = json.load(f)\n",
        "        return pd.json_normalize(data=j[\"data\"]).drop(\n",
        "            [\"created_at\", \"edit_history_tweet_ids\"], axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "# 上記2関数を組み合わせてcorpusを用意\n",
        "def prepareCorpus(path_list):\n",
        "    res = pd.DataFrame()\n",
        "    for path in path_list:\n",
        "        df = JSONtoDF(path)\n",
        "        df[\"preprocessedText\"] = [preprocessing(text) for text in df[\"text\"]]\n",
        "        res = pd.concat([res, df])\n",
        "    return res.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def get_context(tokens=[], target_position=0, sequence_length=128):\n",
        "    token_length = len(tokens)\n",
        "    diff_length = max([0, sequence_length - token_length])\n",
        "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
        "    half_size = int(sequence_length / 2)\n",
        "    if diff_length:\n",
        "        return tokens + diff_length * [0], target_position\n",
        "\n",
        "    if half_size > target_position:\n",
        "        return tokens[0:sequence_length], target_position\n",
        "    else:\n",
        "        return (\n",
        "            tokens[\n",
        "                target_position\n",
        "                - half_size : target_position\n",
        "                - half_size\n",
        "                + sequence_length\n",
        "            ],\n",
        "            half_size,\n",
        "        )\n",
        "\n",
        "\n",
        "def get_usage(\n",
        "    text_list=[],\n",
        "    target_word=None,\n",
        "    output_path=\"word-vectors/{}.dict\".format(random.randrange(1000, 10000)),\n",
        "    sequence_length=256,\n",
        "    buffer_size=512,\n",
        "    layer_range=(0, 12),\n",
        "):\n",
        "    print(\"------------------------\")\n",
        "    print(\"Start process : {}\".format(output_path.split(\"/\")[-1]))\n",
        "\n",
        "    # check params\n",
        "    if not target_word or not len(text_list):\n",
        "        print(\"Exit from function due to improper parameters\")\n",
        "        return\n",
        "\n",
        "    # check outputs\n",
        "    if os.path.exists(\"/content/\" + output_path):\n",
        "        print(\"Exit from function because the file already exists\")\n",
        "        return\n",
        "\n",
        "    # initialize\n",
        "    TW_batches = []\n",
        "    surrounding_words = []\n",
        "    TW_token = tokenizer.encode(target_word)  # TargetWord_token\n",
        "    TW_token = TW_token[1 : len(TW_token) - 1]\n",
        "\n",
        "    for text in tqdm(text_list):\n",
        "        tokens = tokenizer.encode(text)\n",
        "        if not len(TW_token) or TW_token[0] not in tokens:\n",
        "            continue\n",
        "\n",
        "        # このループで取り扱っている一文に対象の単語が含まれている場合以下の処理を実行する\n",
        "        TW_input_ids, TW_position = get_context(\n",
        "            tokens, tokens.index(TW_token[0]), sequence_length\n",
        "        )\n",
        "        TW_batches.append(\n",
        "            {\n",
        "                \"word\": target_word,\n",
        "                \"position\": (TW_position, TW_position + len(TW_token)),\n",
        "                \"input_ids\": TW_input_ids,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 周辺単語についてもtokenizer.tokenizeで文を分かち書きし、必要な単語だけ重複なくListに格納\n",
        "        [\n",
        "            surrounding_words.append(w)\n",
        "            for w in tokenizer.tokenize(text)\n",
        "            if w not in stop_words and w not in surrounding_words\n",
        "        ]\n",
        "\n",
        "        if len(TW_batches) >= buffer_size:\n",
        "            break\n",
        "\n",
        "    usages = {\"target_word\": {}, \"surrounding_words\": [], \"all\": []}\n",
        "\n",
        "    # target_wordの分散表現を取得\n",
        "    with torch.no_grad():\n",
        "        TW_input_ids = torch.tensor([b[\"input_ids\"] for b in TW_batches])\n",
        "        if torch.cuda.is_available():\n",
        "            TW_input_ids = TW_input_ids.cuda()\n",
        "        TW_outputs = model(TW_input_ids, output_hidden_states=True)\n",
        "    TW_hidden_states = np.stack([v.detach().cpu().numpy() for v in TW_outputs[2]])\n",
        "    # TW_outputs[2]は13(隠れ12層+最終層)×文章数×単語数×768次元になる (例： 13, 512, 256, 768)\n",
        "    # 文章の分散表現 : defaultで12層すべての和をとる(TW_vectorsは最終的に512×768次元になる)\n",
        "    TW_vectors = np.sum(\n",
        "        TW_hidden_states[layer_range[0] : layer_range[1], :, :, :], axis=0\n",
        "    ) / (layer_range[1] - layer_range[0])\n",
        "    TW_vectors = np.stack(\n",
        "        [\n",
        "            np.sum(TW_vectors[i, b[\"position\"][0] : b[\"position\"][1], :], axis=0)\n",
        "            / (b[\"position\"][1] - b[\"position\"][0])\n",
        "            for i, b in enumerate(TW_batches)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    usages[\"target_word\"] = {\n",
        "        \"word\": target_word,\n",
        "        \"vector\": np.sum(TW_vectors, axis=0) / len(TW_vectors),\n",
        "    }\n",
        "    usages[\"all\"].append(usages[\"target_word\"])\n",
        "\n",
        "    # target_wordの周辺単語についても分散表現を取得\n",
        "    SW_batches = []\n",
        "    for w in surrounding_words:\n",
        "        SW_token = tokenizer.encode(w)\n",
        "        SW_input_ids, SW_position = get_context(SW_token, 1, 10)\n",
        "        SW_batches.append(\n",
        "            {\n",
        "                \"word\": w,\n",
        "                \"position\": (SW_position, SW_position + len(SW_token) - 2),\n",
        "                \"input_ids\": SW_input_ids,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        SW_input_ids = torch.tensor([s[\"input_ids\"] for s in SW_batches])\n",
        "        if torch.cuda.is_available():\n",
        "            SW_input_ids = SW_input_ids.cuda()\n",
        "        SW_outputs = model(SW_input_ids)\n",
        "        SW_hidden_states = SW_outputs.last_hidden_state.detach().cpu().numpy()\n",
        "    [\n",
        "        usages[\"surrounding_words\"].append(\n",
        "            {\n",
        "                \"word\": s[\"word\"],\n",
        "                \"vector\": np.sum(\n",
        "                    SW_hidden_states[i, s[\"position\"][0] : s[\"position\"][1], :], axis=0\n",
        "                )\n",
        "                / (s[\"position\"][1] - s[\"position\"][0]),\n",
        "            }\n",
        "        )\n",
        "        for i, s in enumerate(SW_batches)\n",
        "    ]\n",
        "\n",
        "    usages[\"all\"].extend(usages[\"surrounding_words\"])\n",
        "\n",
        "    print(\"Finish process : {} / {}\".format(target_word, len(TW_batches)))\n",
        "\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        pickle.dump(usages, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主要処理"
      ],
      "metadata": {
        "id": "2qte-vsL-iYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_xsCuP8UdCB"
      },
      "outputs": [],
      "source": [
        "# ターゲットとする単語や期間の整理は予めここで行う\n",
        "oparation = [\n",
        "    {\n",
        "        \"word\": w,\n",
        "        \"year\": y,\n",
        "        \"path_list\": list(filter(lambda x: w in x and str(y) in x, path_list)),\n",
        "    }\n",
        "    for w in target_words\n",
        "    for y in range(2007, 2021)\n",
        "]\n",
        "for o in oparation:\n",
        "    if not len(o[\"path_list\"]):\n",
        "        continue\n",
        "    output_path = (\n",
        "        base_dir + \"word-vectors/\" + o[\"word\"] + \"-\" + str(o[\"year\"]) + \".dict\"\n",
        "    )\n",
        "    corpus = prepareCorpus(o[\"path_list\"])\n",
        "    get_usage(\n",
        "        text_list=corpus[\"preprocessedText\"].values.tolist(),\n",
        "        target_word=o[\"word\"],\n",
        "        output_path=output_path,\n",
        "        sequence_length=256,\n",
        "        buffer_size=1536,\n",
        "        layer_range=(0, 12),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保存後処理"
      ],
      "metadata": {
        "id": "kgnOijt3M5A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# formatter\n",
        "!pip install -q black[jupyter]\n",
        "!black \"/content/drive/MyDrive/Colab Notebooks/bertClustering.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901d471e-0fe9-48cd-eb4e-a32185c2d764",
        "id": "pkCcMEOnM_PF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### その他"
      ],
      "metadata": {
        "id": "ezlvBruHUPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "import tensorflow as tf\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# RAMのサイズをcheck\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print(\"Your runtime has {:.1f} gigabytes of available RAM\\n\".format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print(\"Not using a high-RAM runtime\")\n",
        "else:\n",
        "    print(\"You are using a high-RAM runtime!\")\n",
        "# GPUの数をcheck\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
        "# Check GPU recognized\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d010c7-0dac-4ffe-f479-515c2e94356d",
        "id": "KyONTzTQUY6H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n",
            "Num GPUs Available:  1\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6354157854472975992\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11586961408\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14767642777879333371\n",
            "physical_device_desc: \"device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "VFXH26A8-biY",
        "_SSWFYtI-6TL",
        "ezlvBruHUPFG"
      ],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}